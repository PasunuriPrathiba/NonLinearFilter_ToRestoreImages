# -*- coding: utf-8 -*-
"""Assignemnt2_IP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H22HcpRJBWO1FVzSdRz2_e16srt8XIZs

2. Prepare the training dataset: \\
a. Select the largest odd window size W, e.g. 13 or 27 \\
b. Prepare a few blur kernels and noise models \\
c. For each training image \\
  i. Degrade multiple times using different blur kernels and noise models \\
  ii. Display a few images to check if the degradation is realistic looking instead of too much or
too little \\
  iii. For each degraded image version,
Mine and store degraded patches of size WxW and central pixel of original patch
"""

#Import required libraries

import skimage.io
import skimage.filters
import sys
import skimage.transform 
import cv2
import numpy as np
from PIL import Image
import requests
import os
import matplotlib.pyplot as plt
from matplotlib import image

#Mount GDrive for reading images stored in GDrive

from google.colab import drive
drive.mount('/content/gdrive')

#Natural Images taken by phone camera called testing images
# Get image names from a GDrive directory

path = "./gdrive/MyDrive/natural"

imageNames1 = []                      #Creating an empty list to store image names
for i in os.scandir(path):            #Looping over the all images in path
  imageNames1.append(i.path)          #appending it empty list with name imageNames1 

imageNames1

#Images taken from mentioned url called training images
# Get image names from a GDrive directory

path = "./gdrive/MyDrive/images"

imageNames = []                     #Creating an empty list to store image names
for i in os.scandir(path):          #Looping over the all images in path
  imageNames.append(i.path)         #appending it empty list with name imageNames 

imageNames

#Resizng the testing images and storing in images1

images1 = []
for i in range(0,2):
  OriginalImage = image.imread(imageNames1[i])
  ResizedImage = skimage.transform.resize(        #Resizing the image
    OriginalImage, (210, 210)) 
  images1.append(ResizedImage)
# i in range(0,2):  
plt.imshow(images1[0])                          #To display image
plt.show()

# Read one image from training and display it

images = []                               #Creating an empty list to store images

for i in range(0,50):
  images.append(
      image.imread(imageNames[i]))        #appending images to empty list with name images 
c = 30                                    #Displaying 31st image
print(images[c].dtype)                    #Printing image datatype
print(images[c].shape)                    #Printing image shape
plt.imshow(images[c])                     #To display image
plt.show()

#Creating a function to degrade an image

def degradedImage(img):
  OriginalImage = skimage.io.imread(img)      #Read the image from filename
  ResizedImage = skimage.transform.resize(
    OriginalImage, (210, 210))                #Resize the image into 210 by 210



  blurred1 = cv2.GaussianBlur(                #Image blurring 1st time using GaussianBlur 
      ResizedImage, (21, 21), 1, 1,           #with kernel size=21*21
      borderType=cv2.BORDER_REPLICATE)



  noisyimage1 = skimage.util.random_noise(     #Adding gaussian noise to the image 1st time
      blurred1, mode = 'gaussian', clip = True,
      mean = 0, var = 0.01)

  blurred2 = cv2.GaussianBlur(                 #Image blurring 2nd time using GaussianBlur ith kernel size=21*21
      noisyimage1, (21, 21), 1, 1,
      borderType=cv2.BORDER_REPLICATE)


  noisyimage2 = skimage.util.random_noise(     #Adding gaussian noise to the image 1st time
      blurred2, mode = 'gaussian', clip = True,
      mean = 0, var = 0.01)
  return ResizedImage,blurred1,noisyimage1,blurred2,noisyimage2

#Creating a function to display degarded images

def ToDisplay(ResizedImage,blurred1,noisyimage1,blurred2,noisyimage2):
  fig = plt.figure(figsize=(10, 10))                  # create figure
  rows = 2                                            # setting values to rows and column variables
  columns = 3

  fig.add_subplot(rows, columns, 1)                   # Adds a subplot at the 1st position
  skimage.io.imshow(ResizedImage)                     # showing image
  plt.axis('off')
  plt.title("Resized Original Image")

  fig.add_subplot(rows, columns, 2)                   # Adds a subplot at the 2nd position
  plt.imshow(blurred1)                                # showing image 
  plt.axis('off')
  plt.title("Blurred Image1")

  fig.add_subplot(rows, columns, 3)                   # Adds a subplot at the 3rd position
  skimage.io.imshow(noisyimage1)                      # showing image
  plt.axis('off')
  plt.title("Noisy Image1")

  fig.add_subplot(rows, columns, 4)                   # Adds a subplot at the 4th position
  plt.imshow(blurred2)                                # showing image
  plt.axis('off')
  plt.title("Blurred Image2")

  fig.add_subplot(rows, columns, 5)                   # Adds a subplot at the 5th position
  skimage.io.imshow(noisyimage2)                      # showing image
  plt.axis('off')
  plt.title("Noisy Image2")

#Creating a function for getting degraded images patches and central pixels of original image

def PatchesMaking(noisyimage2,ResizedImage):
  DegradedPatch = skimage.util.view_as_windows(            # Degraded Patch of size 21*21
      noisyimage2, (21, 21, 1), step=1)
  type(DegradedPatch)                                      # Gives the datatype
  DegradedPatch[0,0,0].shape                               # Gives the shape of each degraded Patch

  OriginalPatch = skimage.util.view_as_windows(            # Original Patch of size 21*21
      ResizedImage, (21, 21, 1), step=1)
  
  list1 = []                                               #creating empty list to store patches
  for i in range(0,190):
    for j in range(0,190):
      for k in range(0,3):
        list1.append(DegradedPatch[i,j,k])                 # appending the all Degraded Patches to list1
        
  list2 = []                                               #creating empty list to store central pixel values
  CentralPixel = np.zeros(shape=(190,190,3))               # Taking CentralPixel as an array of all zeros
  for i in range(0,190):
    for j in range(0,190):
      for k in range(0,3):
        CentralPixel[i,j,k] = OriginalPatch[i,j,k][11,11]  # All central pixels of original patch
        list2.append(CentralPixel[i,j,k])                  #appending central pixel values to list2

  return list1, list2

#Calling all the three functions on images to prepare training data

l1 = []                                 #Temporary variables
l2 = []                                 #Temporary variables
k1 = []                                 #Traning data degraded patches as input
k2 = []                                 #Traing data central pixel of original data as output

for a in range(0,50):                    #Taking 0 images for training data
  #Degrading the 50 images
  [ResizedImage,blurred1,noisyimage1,blurred2,noisyimage2] = degradedImage(imageNames[a])
  #Displaying 50 degraded images
  ToDisplay(ResizedImage,blurred1,noisyimage1,blurred2,noisyimage2)
  #Making patches for 50 degraded images and storing central pixels
  [list1, list2] = PatchesMaking(noisyimage2,ResizedImage)
  
  l1.append(list1[10000:12000])                      #appedning all the degraded patches in listnamely l1
  l2.append(list2[10000:12000])                      #appending all the central pixels in listnamely l2
  k1 = k1+l1[a]                         #storing all the degraded patches in listnamely k1
  k2 = k2+l2[a]                         #storing all the central pixels in listnamely k2
print(len(k1), len(k2))                 #Printing the length of k1 and k2
                                        #Both should be equal to train an ML model

#Calling all the three functions on images to prepare testing data

s1 = []                                               #Temporary variables
s2 = []                                               #Temporary variables
t1 = []                                               #Testing data degraded patches as input
t2 = [ ]                                              #Testing data central pixel of original data as output

for a in range(10, 13):                               #Taking 3 images for training data
  #Degrading the 3 images
  [ResizedImage,blurred1,noisyimage1,blurred2,noisyimage2] = degradedImage(imageNames[a])
  #Displaying 3 degraded images
  ToDisplay(ResizedImage,blurred1,noisyimage1,blurred2,noisyimage2)
  #Making patches for 3 degraded images and storing central pixels
  [list1, list2] = PatchesMaking(noisyimage2,ResizedImage)
  
  s1.append(list1)                                    #appedning all the degraded patches in listnamely s1
  s2.append(list2)                                    #appending all the central pixels in listnamely s2

for y in range(0,2):
  [list1, list2] = PatchesMaking(images1[y],images1[y])
  s1.append(list1)
  s2.append(list2)
  #ToDisplay(images1[y],images1[y],images1[y],images1[y],images1[y])
print(len(s1), len(s2))  

for z in range(0,5):
  t1 = t1+s1[z]                                       #storing all the degraded patches in listnamely t1
  t2 = t2+s2[z]                                       #storing all the central pixels in listnamely t2
print(len(t1), len(t2))                               #Printing the length of t1 and t2
                                                      #Both should be equal to train an ML model

"""
3. Train a regression model: \\
a. Select a window size w less than or equal to the largest window size W \\
b. Select a machine learning model (nonlinear regression), e.g. support vector regression, random
forest regression, neural network regression, or convolutional neural network \\
c. Write a function to read only the w×w central pixels as input, and (optionally) pre-process them (e.g.
make it zero mean and unit variance, or work in HSI space) \\
d. In python, train a regression model to predict the clean (optionally, normalized) central pixel \\
e. Monitor the normalized mean square error or mean absolute error for validation data \\
f. Observe if models over-fits. If so, then implement early stopping \\
g. Experiment with different choices, e.g. window size, machine learning models, capacity of models
(e.g. tree depth, SVR penalty, number of hidden nodes in NN, number of layers and kernels in CNN,
etc.) to find a reasonable model with small normalized RMSE, e.g. less than 1% or 2%. """

#Required libraries

w =21
import pandas as pd
import numpy as np
import tensorflow as tf
from   tensorflow.keras import *
from   tensorflow.keras.models import *
from   tensorflow.keras.layers import *
from   tensorflow.keras.callbacks import *

#Creating class to build regression model
#Model 1
global k1
global k2
global t1
global t2
global imageNames

class regression_model(Model):
    def __init__(self):
        super(regression_model,self).__init__()
        self.dense1 = Dense(units=1000, activation=tf.keras.activations.relu)
        self.dense2 = Dense(units=441, activation=tf.keras.activations.relu)
        self.dense3 = Dense(units=200, activation=tf.keras.activations.relu)
        self.dense4 = Dense(units=1,   activation=tf.identity)

    @tf.function
    def call(self,x):
        h1 = self.dense1(x)
        h2 = self.dense2(h1)
        h3  = self.dense3(h2)
        u = self.dense4(h3) # Output
        return u

#Reshaping the testing data

if __name__=="__main__":
  
  X_test = t1                                             #DegradedPatch
  y_test = t2                                             #CentralPixel

  X_test = tf.constant(X_test,dtype=tf.float32)           #Define as TensorFlow constant
  y_test = tf.constant(y_test,dtype=tf.float32)           #Define as TensorFlow constant

  NUM_SAMPLES = len(t1)                                  #len(t1)
  NUM_VALUES_IN_1SAMPLE = 21*21                           #len(DegradedPatch[0,0,0])
  X_test = tf.reshape(                                    #reshaping 
      X_test,(NUM_SAMPLES,NUM_VALUES_IN_1SAMPLE))

#Reshaping the training data and creating the regression model

if __name__=="__main__":
  
  input = k1                                           #DegradedPatch
  output = k2                                          #CentralPixel

  input = tf.constant(input,dtype=tf.float32)          #Define as TensorFlow constant
  output = tf.constant(output,dtype=tf.float32)        #Define as TensorFlow constant

  NUM_SAMPLES = len(k1)                              #len(k1)
  NUM_VALUES_IN_1SAMPLE = 21*21                        #len(DegradedPatch[0,0,0])
  input = tf.reshape(                                  #reshaping 
      input,(NUM_SAMPLES,NUM_VALUES_IN_1SAMPLE))

  model = regression_model()                           #calling the regression_model
  model.compile(loss=tf.losses.MeanSquaredError(),     #compiling the model
                  optimizer=tf.optimizers.Adam(1e-3))
    
  history = model.fit(                                 #fitting the model
      x=input,y=output, batch_size=len(input),
      epochs=100,validation_split=0.4,
                      verbose=1)

"""From epoch 96 to 100 the val_loss has increased, so we have to stop at 98 epoch."""

#Predicting the central pixel using the trained ML model on training data
#On Vaalidation images

y_pred = model.predict(x=input[10000:20000], batch_size=len(input[108300*4:108300*5]), steps=1)

#Displaying the cleaned image obtained from ML model
#On Validation  data taking few degraded patches randomly from training data set

print(type(y_pred))
print(y_pred.shape)
PredictedImageDNN = np.resize(y_pred, (190, 190, 3))
print(type(PredictedImageDNN))
im = plt.imshow(PredictedImageDNN)

#Creating Random forset
#Model 2 

X = input
y = output
print(len(X))

# Fitting Random Forest Regression to the dataset
# import the regressor
from sklearn.ensemble import RandomForestRegressor
  
 # create regressor object
regressor = RandomForestRegressor(n_estimators = 20, random_state = 0)
  
# fit the regressor with x and y data
regressor.fit(X, y)

X_val = input[10000:20000]
y_val = output[10000:20000]

#Predicting the central pixel using the trained ML model
#On validation data

y_predv = regressor.predict(np.array(X_val)) # test the output by changing values

#Displaying cleaned image obtained from ML model
#On Validation data taking some degraded patches randomly from training data

print(type(y_predv))
print(y_predv.shape)
PredictedImageRF = np.resize(y_predv, (190, 190, 3))
print(type(PredictedImageRF))
im = plt.imshow(PredictedImageRF)

#Getting metrics from model
#on validation data

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, y_predv))
print('Mean Squared Error:', metrics.mean_squared_error(y_val, y_predv))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, y_predv)))

"""4. Apply the model on held-out or private images (couple of them synthetically degraded, couple of them real images with slightly poor quality): \\
a. For each degraded testing image \\
i. Initialize blank clean image \\
ii. For each overlapping patch of w opt ×w opt \\
1. Predict clean central pixel \\
2. Optionally denormalize (multiply the std deviation, add the mean, or inverse HSI) \\
iii. Contrast enhance using a function call (no need to implement from scratch) and put the
pixels in the right range (0-255, integer) \\
iv. Display the cleaned image
"""

#Predicting the central pixel using the trained DNN model
#Testing data

y_predT = model.predict(X_test[108300*3:108300*4], batch_size=len(X_test[108300*3:108300*4]), steps=1)

#Displaying the cleaned image obtained from DNN model
#Testing data

print(type(y_predT))
print(y_predT.shape)
PredictedImageDNNT = np.resize(y_predT, (190, 190, 3))
print(type(PredictedImageDNNT))
im = plt.imshow(PredictedImageDNNT)

#Predicting the central pixel using the trained RandomForest model
#Testing data

y_predRFT = regressor.predict(np.array(X_test[108300*3:108300*4]))#.reshape(1, 1)) # test the output by changing values
y_testRFT = y_test[108300*3:108300*4]

#Displaying cleaned image obtained from RandomForest model
#Testing data

print(type(y_predRFT))
print(y_predRFT.shape)
PredictedImageRFT = np.resize(y_predRFT, (190, 190, 3))
print(type(PredictedImageRFT))
im = plt.imshow(PredictedImageRFT)

"""From above two images, we can observe RandomForest performing better than NN."""

#Getting metrics from RandomForest model
#on testing data

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_testRFT, y_predRFT))
print('Mean Squared Error:', metrics.mean_squared_error(y_testRFT, y_predRFT))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_testRFT, y_predRFT)))

"""5. Your report or ipython comment cells should contain the following: \\
a. Your design choices: which window size range and ML models you will try \\
b. Your initial guess of what trends you will observe about what will work better \\
c. Your observation, and any surprises in what worked better. E.g., was there a maximum window size
above which you did not get any advantage when you increased the window size further? Did you
expect CNN to be better, but something else turned out to be better? Did you expect a more
complex model to work better? \\
d. Some thoughts on why you think you had any surprising observations. \\
e. References, including internet sources of code, blogs, or friends with whom you discussed. \\

Report: \\

a) Design choices: \\

Window size = 21*21 \\
ML models = NN, RandomForest.

b) Intial Guess: \\

I thought that by appropritley tunning number of nuerons and layers in NN, I would get less loss and good accuracy comapred to RandomForest.

c) Suprise results: \\

By seeing the results from two models, we can observe the RandomForest is working better than NN interms of loss and accuracy.

But for executing the NN takes less time compared with RandomForest with same number of samples.

d) Some thoughts on surprising results: \\

Because of taking optimal number of estimaters in RandomForest, I got less loss and good accuracy. Whereas in NN, the hyper parameters are not tunned properly and it gives poor results than RandomForest.

e) Refernces: \\

https://www.kaggle.com/vbookshelf/cnn-how-to-use-160-000-images-without-crashing

https://www.kaggle.com/questions-and-answers/183996
https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb

https://keras.io/api/losses/regression_losses/#meansquarederror-class

https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/

https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/

https://www.google.com/search?q=how+to+read+image+from+ineternt+in+python&oq=how+to+read+image+from+ineternt+in+python&aqs=chrome..69i57j33i22i29i30l2.12794j1j7&sourceid=chrome&ie=UTF-8

https://www.google.com/search?q=model.fit%28%29+in+keras&ei=5OVdYd7ENv3H4-EP6_qXsA0&oq=model.fit%28%29+in+keras&gs_lcp=Cgdnd3Mtd2l6EAEYAjIFCAAQkQIyBQgAEJECMgUIABCRAjIFCAAQgAQyBQgAEJECMgUIABCABDIFCAAQgAQ6BAgAEEM6CwgAEIAEELEDEIMBOhQIABCABBCxAxCDARCLAxDSAxCoAzoICAAQsQMQgwE6EQguEIAEELEDEIMBEMcBEKMCOgQILhBDOggILhCABBCxAzoFCC4QgAQ6CAgAEIAEELEDOgsILhCABBDHARCvAUoECEEYAFDkggFY46YBYN66AWgBcAJ4AIABqAOIAbQbkgEJMC4xLjYuNC4xmAEAoAEBsAEAuAECwAEB&sclient=gws-wiz

I have discussed with myclassmates for doing assignment and their names are sayali and samay.
"""